"""Generate ngram probabilities using Witten-Bell smoothing.

Partially generated by ChatGPT 2024-03-21
"""

import math
from collections import Counter, defaultdict
from typing import IO, Any, Final, Union

BOS: Final[str] = "<s>"
EOS: Final[str] = "</s>"
UNK: Final[str] = "<unk>"


class WittenBellNgram:
    """Generate ngram probabilities using Witten-Bell smoothing."""

    def __init__(
        self, sentences: list[list[str]], n: int, unk_logprob: float = -100
    ) -> None:
        """Train on corpus."""
        self.n = n
        self.vocab: set[str] = set()
        self.unk_logprob = unk_logprob
        self.ngram_counts: list[Any] = [defaultdict(Counter) for _ in range(n)]
        self._train(sentences)

    def _train(self, corpus) -> None:
        for sentence in corpus:
            tokens = [BOS] + sentence + [EOS]
            self.vocab.update(tokens)
            for i in range(len(tokens)):
                for k in range(1, self.n + 1):
                    if (i + k) <= len(tokens):
                        prefix = tuple(tokens[i : i + k - 1])
                        word = tokens[i + k - 1]
                        self.ngram_counts[k - 1][prefix][word] += 1

    def _get_total_count(self, context):
        return sum(self.ngram_counts[len(context)][context].values())

    def _get_unique_followers(self, context):
        return len(self.ngram_counts[len(context)][context])

    def prob(self, context: Union[list[str], tuple[str, ...]], word: str) -> float:
        """Get probability of word given context."""
        context = tuple(context)
        if len(context) > self.n - 1:
            context = context[-(self.n - 1) :]

        count_hw = self.ngram_counts[len(context)][context][word]
        total = self._get_total_count(context)
        unique = self._get_unique_followers(context)

        if count_hw > 0:
            return count_hw / (total + unique)

        lambda_ = unique / (total + unique) if (total + unique) > 0 else 1.0
        backoff_context = context[1:] if context else ()
        return lambda_ * self.prob(backoff_context, word)

    def log10_prob(
        self, context: Union[list[str], tuple[str, ...]], word: str
    ) -> float:
        """Get log10 probability of word given context."""
        prob = self.prob(context, word)
        return math.log10(prob) if prob > 0 else float("-inf")

    def log10_backoff_weight(self, context: Union[list[str], tuple[str, ...]]) -> float:
        """Get log10 backoff weight for context."""
        context = tuple(context)
        total = self._get_total_count(context)
        unique = self._get_unique_followers(context)
        if total + unique == 0:
            return 0.0
        weight = unique / (total + unique)
        return math.log10(weight) if weight > 0 else float("-inf")

    def to_arpa(self, arpa_file: IO[str]) -> None:
        """Write ARPA language model to file."""
        order = self.n

        # \data\ header
        print("\\data\\", file=arpa_file)
        for i in range(order):
            ngram_count = sum(len(v) for v in self.ngram_counts[i].values())
            if i == 0:
                # <unk>
                ngram_count += 1

            print(f"ngram {i+1}={ngram_count}", file=arpa_file)

        print("", file=arpa_file)

        for i in range(order):
            print(f"\\{i+1}-grams:", file=arpa_file)
            if i == 0:
                print(f"{self.unk_logprob}\t{UNK}", file=arpa_file)

            for context, counter in self.ngram_counts[i].items():
                for word in counter:
                    full_ngram = context + (word,)

                    # --- Filter invalid n-grams for ARPA format ---
                    if BOS in full_ngram[1:]:  # <s> must only be first
                        continue
                    if EOS in full_ngram[:-1]:  # </s> must only be final
                        continue

                    # Special case: <s> unigram gets -99 log-prob
                    if (i == 0) and (word == BOS):
                        log10_p = -99.0
                        bow = self.log10_backoff_weight((word,))
                        line = f"{log10_p:.7f}\t{word}\t{bow:.7f}"
                    else:
                        log10_p = self.log10_prob(context, word)
                        line = f"{log10_p:.7f}\t{' '.join(full_ngram)}"

                        # Add backoff weight if not final order
                        if i < (order - 1):
                            bow = self.log10_backoff_weight(full_ngram)
                            if bow != 0:
                                line += f"\t{bow:.7f}"

                    print(line, file=arpa_file)

            print("", file=arpa_file)

        print("\\end\\", file=arpa_file)
